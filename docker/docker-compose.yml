name: "spark-ecosystem-cluster"

networks:
  spark-network:
    name: spark-network
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: "172.19.0.0/16"

services:
  spark-master:
    image: my-spark-cluster:3.5.0
    ports:
      - "9090:8080"
      - "7077:7077"
      - "4040:4040"
    volumes:
      - ./spark/apps:/opt/spark-apps
      - ./spark/data:/opt/spark-data
      - ./spark/metrics.properties:/opt/spark/conf/metrics.properties
      - ./spark/config/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    environment:
      - SPARK_LOCAL_IP=172.19.0.10
      - SPARK_WORKLOAD=master
      - SPARK_DAEMON_MEMORY=3G
    networks:
      spark-network:
        ipv4_address: 172.19.0.10

  spark-worker-a:
    image: my-spark-cluster:3.5.0
    ports:
      - "9091:8080"
      - "7000:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=3
      - SPARK_WORKER_MEMORY=3G
      - SPARK_EXECUTOR_MEMORY=3G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=172.19.0.2
    volumes:
      - ./spark/apps:/opt/spark-apps
      - ./spark/data:/opt/spark-data
      - ./spark/metrics.properties:/opt/spark/conf/metrics.properties
      - ./spark/config/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf

    networks:
      spark-network:
        ipv4_address: 172.19.0.2

  spark-worker-b:
    image: my-spark-cluster:3.5.0
    ports:
      - "9093:8080"
      - "7001:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=3
      - SPARK_WORKER_MEMORY=3G
      - SPARK_EXECUTOR_MEMORY=3G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=172.19.0.3
    volumes:
      - ./spark/apps:/opt/spark-apps
      - ./spark/data:/opt/spark-data
      - ./spark/metrics.properties:/opt/spark/conf/metrics.properties
      - ./spark/config/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf

    networks:
      spark-network:
        ipv4_address: 172.19.0.3

  kafka:
    image: bitnami/kafka:3.7.0
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_CFG_NODE_ID: 0
      KAFKA_CFG_PROCESS_ROLES: controller,broker
      KAFKA_CFG_LISTENERS: "PLAINTEXT://172.19.0.4:9092,CONTROLLER://0.0.0.0:9093"
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
      KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: 0@172.19.0.4:9093
      KAFKA_CFG_CONTROLLER_LISTENER_NAMES: CONTROLLER
    networks:
      spark-network:
        ipv4_address: 172.19.0.4

  init-kafka:
    image: bitnami/kafka:3.7.0
    container_name: init-kafka
    depends_on:
      kafka:
        condition: service_started
    entrypoint: [ '/usr/bin/bash', '-c' ]
    command: |
      "
      set -ex

      # blocks until kafka is reachable
      /opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server 172.19.0.4:9092 --list
      
      echo -e 'Creating kafka topics'
      /opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server 172.19.0.4:9092 --create --if-not-exists --topic topic-events --replication-factor 1 --partitions 1

      echo -e 'Successfully created the following topics:'
      /opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server 172.19.0.4:9092 --list
      "
    networks:
      spark-network:
        ipv4_address: 172.19.0.21

  localstack:
    container_name: "${LOCALSTACK_DOCKER_NAME:-localstack-main}"
    image: localstack/localstack:3.4.0
    ports:
      - "127.0.0.1:4566:4566"
      - "127.0.0.1:4510-4559:4510-4559"
    environment:
      - SERVICES=s3
      - DEBUG=true
      - DEFAULT_REGION=eu-west-1
      - AWS_ACCESS_KEY_ID=test
      - AWS_SECRET_ACCESS_KEY=test
    volumes:
      - "${LOCALSTACK_VOLUME_DIR:-./volume}:/var/lib/localstack"
      - "/var/run/docker.sock:/var/run/docker.sock"
    networks:
      spark-network:
        ipv4_address: 172.19.0.6

  init-s3-storage:
    container_name: init-s3-storage
    image: localstack/localstack:3.4.0
    entrypoint: [ "bash", "-c", "awslocal --endpoint-url http://172.19.0.6:4566 s3 mb s3://my-bucket" ]
    depends_on:
      localstack:
        condition: service_healthy
    networks:
      spark-network:
        ipv4_address: 172.19.0.7

  postgres:
    image: postgres:14-alpine
    container_name: postgres
    environment:
      - POSTGRES_USER
      - POSTGRES_PASSWORD
      - POSTGRES_DB
    healthcheck:
      test: [ "CMD", "psql", "-U", "${POSTGRES_USER}", "${POSTGRES_DB}" ]
    ports:
      - '5432:5432'
    networks:
      spark-network:
        ipv4_address: 172.19.0.8

  hive-metastore:
    image: apache/hive:4.0.0-alpha-2
    container_name: hive-metastore
    environment:
      - SERVICE_NAME=metastore
      - DB_DRIVER=postgres
      - HIVE_CUSTOM_CONF_DIR=/hive_custom_conf
    ports:
      - "9083:9083"
    volumes:
      - ./data/delta/osdp/spark-warehouse:/opt/spark/work-dir/data/delta/osdp/spark-warehouse
      - ./hive/conf/hive-site.xml:/opt/hive/conf/hive-site.xml
      - ./hive/conf/jars/hadoop-aws-3.2.2.jar:/opt/hive/lib/hadoop-aws-3.2.2.jar
      - ./hive/conf/jars/aws-java-sdk-bundle-1.11.375.jar:/opt/hive/lib/aws-java-sdk-bundle-1.11.375.jar
      - ./hive/conf/.hiverc:/opt/hive/conf/.hiverc

    depends_on:
      postgres:
        condition: service_healthy
    networks:
      spark-network:
        ipv4_address: 172.19.0.9

  spark-thrift-server:
    image: my-spark-cluster:3.5.0
    container_name: spark-thrift-server
    depends_on:
      spark-master:
        condition: service_started
      hive-metastore:
        condition: service_started
    environment:
      - SPARK_WORKLOAD=thrift-server
      - HIVE_METASTORE_URI=thrift://hive-metastore:9083
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_SQL_HIVE_METASTORE_URIS=thrift://hive-metastore:9083
    ports:
      - "10000:10000"
    entrypoint: >
      /opt/spark/bin/spark-submit
        --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2
        --master spark://spark-master:7077
        --deploy-mode client
        --executor-memory 1G
        --driver-memory 1G
        --total-executor-cores 2
        --conf spark.sql.hive.metastore.version=2.3.9
        --conf spark.sql.uris=thrift://hive-metastore:9083
        --conf spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083
        --conf spark.hadoop.fs.s3a.endpoint=http://localstack:4566
        --conf spark.hadoop.fs.s3a.access.key=test
        --conf spark.hadoop.fs.s3a.secret.key=test
        --conf spark.hadoop.fs.s3a.path.style.access=true
        --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
        --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
        --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
        local://opt/spark/jars/spark-hive-thriftserver_2.12-3.5.0.jar
    networks:
      spark-network:
        ipv4_address: 172.19.0.11

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    networks:
      spark-network:
        ipv4_address: 172.19.0.13

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "19090:9090"
    networks:
      spark-network:
        ipv4_address: 172.19.0.12
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
